{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deteccion de Armas</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recursos y Herramientas Usadas </h3>\n",
    "<br>\n",
    "<li>\n",
    "    <a href=\"https://blog.roboflow.com/train-a-tensorflow2-object-detection-model/\">tensorflow2</a> \n",
    "    <p>Conceptos sobre tensorflow2</p>\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://repo.anaconda.com/archive/\">Anaconda3</a> \n",
    "    <p>Versión usada Anaconda3 (Anaconda3-2019.07-Windows-x86_64)</p>\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://visualstudio.microsoft.com/es/vs/community/\">Visual</a> \n",
    "    <p> Instalamos - Desarrollo para el escritorio con c++</p> <br>\n",
    "    <img src=\"Tensorflow/c++.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal\">NVIDIA CUDA toolkit</a>\n",
    "    <p>Instalamos NVIDIA CUDA toolkit que nos ayuda acelerar la computación hasta cuatro veces</p><br>\n",
    "    <img src=\"Tensorflow/CUDA 10.1.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://developer.nvidia.com/rdp/cudnn-archive\">NVIDIA cuDNN</a>\n",
    "    <p>Es una biblioteca de primitivas acelerada por GPU para redes neuronales profundas.</p>\n",
    "    <img src=\"Tensorflow/cuDNN.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://github.com/protocolbuffers/protobuf/releases\">Protobuf</a>\n",
    "    <p>Es un formato de intercambio de datos desarrollado originalmente para uso interno en Google y lanzado al gran público por la empresa en 2008 como proyecto de código abierto (en parte, con licencia Apache 2.0).</p>\n",
    "    <br>\n",
    "    <img src=\"Tensorflow/Protocol.PNG\" alt=\"Italian Trulli\">\n",
    "    <br>\n",
    "    <img src=\"Tensorflow/Protocol2.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://github.com/tensorflow/models\"> TensorFlow Model Garden</a>\n",
    "  <p>Es un repositorio con varias implementaciones diferentes de modelos y soluciones de modelado de última generación (SOTA) para los usuarios de TensorFlow. Nuestro objetivo es demostrar las mejores prácticas de modelado para que los usuarios de TensorFlow puedan aprovechar al máximo TensorFlow para su investigación y desarrollo de productos.</p>\n",
    "    <img src=\"Tensorflow/GardenTensorFlow.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n",
    "\n",
    "<br>\n",
    "\n",
    "<li>\n",
    "    <a href=\"https://github.com/tzutalin/labelImg\">labelImg</a>\n",
    "    <p>LabelImg es una herramienta de anotación de imágenes gráficas. Está escrito en Python y usa Qt para su interfaz gráfica. Las anotaciones se guardan como archivos XML en formato PASCAL VOC, el formato utilizado por ImageNet . Además, también es compatible con los formatos YOLO y CreateML.</p>\n",
    "        <img src=\"Tensorflow/labelImage.PNG\" alt=\"Italian Trulli\">\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Creamos las rutas para un acceso más cómodo a las carpetas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/workspace\n",
      "Tensorflow/scripts\n",
      "Tensorflow/models\n",
      "Tensorflow/workspace/annotations\n",
      "Tensorflow/workspace/images\n",
      "Tensorflow/workspace/models\n",
      "Tensorflow/workspace/pre-trained-models\n",
      "Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config\n",
      "Tensorflow/workspace/models/my_ssd_mobnet/\n"
     ]
    }
   ],
   "source": [
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "print(WORKSPACE_PATH)\n",
    "SCRIPTS_PATH = 'Tensorflow/scripts'\n",
    "print(SCRIPTS_PATH)\n",
    "APIMODEL_PATH = 'Tensorflow/models'\n",
    "print(APIMODEL_PATH)\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "print(ANNOTATION_PATH)\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "print(IMAGE_PATH)\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "print(MODEL_PATH)\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "print(PRETRAINED_MODEL_PATH)\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "print(CONFIG_PATH)\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Creamos las etiquetas y guardamos en un archivo label_map.pbtxt</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Arma', 'id': 1}]\n"
     ]
    }
   ],
   "source": [
    "labels = [{'name':'Arma', 'id':1}]\n",
    "\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.Luego generar los datos de entrenamiento y pruebas y los almacenamos en un archivo <a href=\"https://www.tensorflow.org/tutorials/load_data/tfrecord\">TFRecord</a></h3>\n",
    "<p>TFRecord es como un formato simple para almacenar una secuencia de registros binarios. Los datos convertidos a este formato ocupan menos espacio en disco, lo que hace que cada operación realizada en el conjunto de datos sea más rápida . También está optimizado para manejar grandes conjuntos de datos dividiéndolo en varios subconjuntos. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Lo siguiente será clonar el repositorio de Model Garden para TensorFlow</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "Updating files:  32% (840/2549)\n",
      "Updating files:  33% (842/2549)\n",
      "Updating files:  34% (867/2549)\n",
      "Updating files:  35% (893/2549)\n",
      "Updating files:  36% (918/2549)\n",
      "Updating files:  37% (944/2549)\n",
      "Updating files:  38% (969/2549)\n",
      "Updating files:  39% (995/2549)\n",
      "Updating files:  40% (1020/2549)\n",
      "Updating files:  41% (1046/2549)\n",
      "Updating files:  42% (1071/2549)\n",
      "Updating files:  43% (1097/2549)\n",
      "Updating files:  44% (1122/2549)\n",
      "Updating files:  45% (1148/2549)\n",
      "Updating files:  46% (1173/2549)\n",
      "Updating files:  47% (1199/2549)\n",
      "Updating files:  48% (1224/2549)\n",
      "Updating files:  49% (1250/2549)\n",
      "Updating files:  50% (1275/2549)\n",
      "Updating files:  51% (1300/2549)\n",
      "Updating files:  52% (1326/2549)\n",
      "Updating files:  53% (1351/2549)\n",
      "Updating files:  54% (1377/2549)\n",
      "Updating files:  55% (1402/2549)\n",
      "Updating files:  56% (1428/2549)\n",
      "Updating files:  57% (1453/2549)\n",
      "Updating files:  58% (1479/2549)\n",
      "Updating files:  59% (1504/2549)\n",
      "Updating files:  60% (1530/2549)\n",
      "Updating files:  61% (1555/2549)\n",
      "Updating files:  62% (1581/2549)\n",
      "Updating files:  63% (1606/2549)\n",
      "Updating files:  64% (1632/2549)\n",
      "Updating files:  65% (1657/2549)\n",
      "Updating files:  66% (1683/2549)\n",
      "Updating files:  67% (1708/2549)\n",
      "Updating files:  68% (1734/2549)\n",
      "Updating files:  69% (1759/2549)\n",
      "Updating files:  70% (1785/2549)\n",
      "Updating files:  71% (1810/2549)\n",
      "Updating files:  72% (1836/2549)\n",
      "Updating files:  73% (1861/2549)\n",
      "Updating files:  74% (1887/2549)\n",
      "Updating files:  75% (1912/2549)\n",
      "Updating files:  76% (1938/2549)\n",
      "Updating files:  77% (1963/2549)\n",
      "Updating files:  78% (1989/2549)\n",
      "Updating files:  79% (2014/2549)\n",
      "Updating files:  80% (2040/2549)\n",
      "Updating files:  81% (2065/2549)\n",
      "Updating files:  82% (2091/2549)\n",
      "Updating files:  83% (2116/2549)\n",
      "Updating files:  84% (2142/2549)\n",
      "Updating files:  84% (2146/2549)\n",
      "Updating files:  85% (2167/2549)\n",
      "Updating files:  86% (2193/2549)\n",
      "Updating files:  87% (2218/2549)\n",
      "Updating files:  88% (2244/2549)\n",
      "Updating files:  89% (2269/2549)\n",
      "Updating files:  90% (2295/2549)\n",
      "Updating files:  91% (2320/2549)\n",
      "Updating files:  92% (2346/2549)\n",
      "Updating files:  93% (2371/2549)\n",
      "Updating files:  94% (2397/2549)\n",
      "Updating files:  95% (2422/2549)\n",
      "Updating files:  96% (2448/2549)\n",
      "Updating files:  97% (2473/2549)\n",
      "Updating files:  98% (2499/2549)\n",
      "Updating files:  99% (2524/2549)\n",
      "Updating files: 100% (2549/2549)\n",
      "Updating files: 100% (2549/2549), done.\n"
     ]
    }
   ],
   "source": [
    "!cd Tensorflow && git clone https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Creamos una carpeta con el nombre de \"my_ssd_mobnet\"</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Lo siguiente sera copiar el archivo pipeline.config y moverlo a la caperta que acabamos de  crear \"my_ssd_mobnet\"</h3>\n",
    "\n",
    "<p>Del repositorio clonado en el punto 4, lo que vamos copiar es la API de detección de objetos Tensorflow (Tensorflow/workspace/pre-trained-models),\n",
    "<a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\">Detection Model Zoo</a> Proporcionamos una colección de modelos de detección entrenados previamente en el conjunto de datos COCO 2017 . Estos modelos pueden ser útiles para inferencias listas para usar si está interesado en categorías que ya están en esos conjuntos de datos<p>\n",
    "<p>También son útiles para inicializar  modelos al entrenar un conjuntos de datos novedosos</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7. Importamos alguna dependencias</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Creamos una ruta de acceso para el archivo pipeline.config \"CONFIG_PATH\"</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'\n",
    "print(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>9. lo siguiente obtener el archivo pipeline.config y guardarlo en una variable \"config\" </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 1\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " }, 'train_config': batch_size: 4\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"detection\"\n",
       " fine_tune_checkpoint_version: V2, 'train_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/train.record\"\n",
       " }, 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false, 'eval_input_configs': [label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }\n",
       " ], 'eval_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. Configuramos el archivo pipeline.config </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = 1\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model {\n",
       "  ssd {\n",
       "    num_classes: 1\n",
       "    image_resizer {\n",
       "      fixed_shape_resizer {\n",
       "        height: 320\n",
       "        width: 320\n",
       "      }\n",
       "    }\n",
       "    feature_extractor {\n",
       "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "      depth_multiplier: 1.0\n",
       "      min_depth: 16\n",
       "      conv_hyperparams {\n",
       "        regularizer {\n",
       "          l2_regularizer {\n",
       "            weight: 3.9999998989515007e-05\n",
       "          }\n",
       "        }\n",
       "        initializer {\n",
       "          random_normal_initializer {\n",
       "            mean: 0.0\n",
       "            stddev: 0.009999999776482582\n",
       "          }\n",
       "        }\n",
       "        activation: RELU_6\n",
       "        batch_norm {\n",
       "          decay: 0.996999979019165\n",
       "          scale: true\n",
       "          epsilon: 0.0010000000474974513\n",
       "        }\n",
       "      }\n",
       "      use_depthwise: true\n",
       "      override_base_feature_extractor_hyperparams: true\n",
       "      fpn {\n",
       "        min_level: 3\n",
       "        max_level: 7\n",
       "        additional_layer_depth: 128\n",
       "      }\n",
       "    }\n",
       "    box_coder {\n",
       "      faster_rcnn_box_coder {\n",
       "        y_scale: 10.0\n",
       "        x_scale: 10.0\n",
       "        height_scale: 5.0\n",
       "        width_scale: 5.0\n",
       "      }\n",
       "    }\n",
       "    matcher {\n",
       "      argmax_matcher {\n",
       "        matched_threshold: 0.5\n",
       "        unmatched_threshold: 0.5\n",
       "        ignore_thresholds: false\n",
       "        negatives_lower_than_unmatched: true\n",
       "        force_match_for_each_row: true\n",
       "        use_matmul_gather: true\n",
       "      }\n",
       "    }\n",
       "    similarity_calculator {\n",
       "      iou_similarity {\n",
       "      }\n",
       "    }\n",
       "    box_predictor {\n",
       "      weight_shared_convolutional_box_predictor {\n",
       "        conv_hyperparams {\n",
       "          regularizer {\n",
       "            l2_regularizer {\n",
       "              weight: 3.9999998989515007e-05\n",
       "            }\n",
       "          }\n",
       "          initializer {\n",
       "            random_normal_initializer {\n",
       "              mean: 0.0\n",
       "              stddev: 0.009999999776482582\n",
       "            }\n",
       "          }\n",
       "          activation: RELU_6\n",
       "          batch_norm {\n",
       "            decay: 0.996999979019165\n",
       "            scale: true\n",
       "            epsilon: 0.0010000000474974513\n",
       "          }\n",
       "        }\n",
       "        depth: 128\n",
       "        num_layers_before_predictor: 4\n",
       "        kernel_size: 3\n",
       "        class_prediction_bias_init: -4.599999904632568\n",
       "        share_prediction_tower: true\n",
       "        use_depthwise: true\n",
       "      }\n",
       "    }\n",
       "    anchor_generator {\n",
       "      multiscale_anchor_generator {\n",
       "        min_level: 3\n",
       "        max_level: 7\n",
       "        anchor_scale: 4.0\n",
       "        aspect_ratios: 1.0\n",
       "        aspect_ratios: 2.0\n",
       "        aspect_ratios: 0.5\n",
       "        scales_per_octave: 2\n",
       "      }\n",
       "    }\n",
       "    post_processing {\n",
       "      batch_non_max_suppression {\n",
       "        score_threshold: 9.99999993922529e-09\n",
       "        iou_threshold: 0.6000000238418579\n",
       "        max_detections_per_class: 100\n",
       "        max_total_detections: 100\n",
       "        use_static_shapes: false\n",
       "      }\n",
       "      score_converter: SIGMOID\n",
       "    }\n",
       "    normalize_loss_by_num_matches: true\n",
       "    loss {\n",
       "      localization_loss {\n",
       "        weighted_smooth_l1 {\n",
       "        }\n",
       "      }\n",
       "      classification_loss {\n",
       "        weighted_sigmoid_focal {\n",
       "          gamma: 2.0\n",
       "          alpha: 0.25\n",
       "        }\n",
       "      }\n",
       "      classification_weight: 1.0\n",
       "      localization_weight: 1.0\n",
       "    }\n",
       "    encode_background_as_zeros: true\n",
       "    normalize_loc_loss_by_codesize: true\n",
       "    inplace_batchnorm_update: true\n",
       "    freeze_batchnorm: false\n",
       "  }\n",
       "}\n",
       "train_config {\n",
       "  batch_size: 4\n",
       "  data_augmentation_options {\n",
       "    random_horizontal_flip {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_crop_image {\n",
       "      min_object_covered: 0.0\n",
       "      min_aspect_ratio: 0.75\n",
       "      max_aspect_ratio: 3.0\n",
       "      min_area: 0.75\n",
       "      max_area: 1.0\n",
       "      overlap_thresh: 0.0\n",
       "    }\n",
       "  }\n",
       "  sync_replicas: true\n",
       "  optimizer {\n",
       "    momentum_optimizer {\n",
       "      learning_rate {\n",
       "        cosine_decay_learning_rate {\n",
       "          learning_rate_base: 0.07999999821186066\n",
       "          total_steps: 50000\n",
       "          warmup_learning_rate: 0.026666000485420227\n",
       "          warmup_steps: 1000\n",
       "        }\n",
       "      }\n",
       "      momentum_optimizer_value: 0.8999999761581421\n",
       "    }\n",
       "    use_moving_average: false\n",
       "  }\n",
       "  fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       "  num_steps: 50000\n",
       "  startup_delay_steps: 0.0\n",
       "  replicas_to_aggregate: 8\n",
       "  max_number_of_boxes: 100\n",
       "  unpad_groundtruth_tensors: false\n",
       "  fine_tune_checkpoint_type: \"detection\"\n",
       "  fine_tune_checkpoint_version: V2\n",
       "}\n",
       "train_input_reader {\n",
       "  label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       "  tf_record_input_reader {\n",
       "    input_path: \"Tensorflow/workspace/annotations/train.record\"\n",
       "  }\n",
       "}\n",
       "eval_config {\n",
       "  metrics_set: \"coco_detection_metrics\"\n",
       "  use_moving_averages: false\n",
       "}\n",
       "eval_input_reader {\n",
       "  label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       "  shuffle: false\n",
       "  num_epochs: 1\n",
       "  tf_record_input_reader {\n",
       "    input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>11. Procedemos a escribir el comando dentro del Notebook </h3>\n",
    "<p>Una vez generado el comando lo pegamos en una terminal para poder ver el estado a medida que se va actualizando, tambie por que alveces falla dentro de un Notebook jupyter</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> La cantidad de pasos de entrenamiento será de 5000, a mas pasos le demos mejor será la presión</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>12. comenzamos con el entrenamiento</h3>\n",
    "<p style=\"color:#00b230\";>Inicio 04:46 p.m - del 04/07/2021</p>  \n",
    "<p style=\"color:#f63232\";>Fin 04:11 a.m - del 05/07/2021</p>  \n",
    "<br>\n",
    "<img src=\"Tensorflow/Inicio.PNG\" alt=\"Italian Trulli\">\n",
    "<br>\n",
    "<h3>Según el registro de entrenamiento concluyó a las 04:11 a.m - del 05/07/2021</h3>\n",
    "<img src=\"Tensorflow/registro.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2071ca40e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos la configuración de la canalización y creamos un modelo de detección\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restauramos el punto de control\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'Arma'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 600)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.5,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>13. Evidencias</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tensorflow/evidence1.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tensorflow/evidence2.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tensorflow/evidence3.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tensorflow/evidence4.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tensorflow/evidence5.PNG\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
